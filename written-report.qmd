---
title: "Predicting a Pokémon's Strength Using Variables Other Than Base Stats"
author: "STA 210 BDN: Nate Krall, Daniel Cohen, Brian Kim"
date: "11/15/2023"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r warning = F, echo = F}
#| label: load packages and data
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(patchwork)
library(knitr)
library(rms)

pokemon <- read.csv("data/pokemon.csv")
pokemon <- pokemon[-774, ]
pokemon$capture_rate <- as.integer(pokemon$capture_rate)
```

# Introduction and Data:

The expansive world of Pokémon, at its core, is a children's game. However, a deeper look at the numbers and statistics the game is built from reveals several intricate relationships among the different pokémons' characteristics. Each creature has its own specific set of base statistics, colloquially referred to as "base stats," including attack, special attack, defense, special defense, and speed, which indicate that pokémon's battle prowess. Summing these stats yields a pokémon's total base stats, which is the best measure of a pokémon's overall strength when all pokémon are put at an even playing field -- common knowledge for any pokémon fan. We are interested in measuring a pokémon's strength without using base stats as predictors, giving us insight on how strong the relationships among pokémon's different characteristics actually are. Thus, we are looking to answer the following research question: **Can we predict a Pokémon's Base Stat Total from other variables?** In other words, we are analyzing how well variables such as the pokémon's type, capture rate, growth rate, generation, height, weight, base happiness, weaknesses, and if the pokemon is legendary or not can predict a pokémon's total base stats. We hypothesize that a multiple linear regression model including some formation of these predictor variables will be a somewhat strong predictor for base_total -- thinking about the game, stronger pokémon would seem to have certain values for these predictor variables when compared to weaker ones: for example, legendary pokémon tend to be stronger in battle than non legendary pokémon, so we might expect is_legendary to be a useful predictor for base_total, for example.

We retrieved the dataset from kaggle.com, a large data science online community, and the dataset is called ["The Complete Pokemon Dataset"](https://labs-az-08.oit.duke.edu:30269/#0) created by Rounak Banik in 2017. The dataset was retrieved via web scraper from the website serebii.net, an all-in-one, reliable data hub for all things pokemon in 2017. Since it was formed in 2017, the dataset does not include pokémon from more recent games, but still includes a whopping 801 pokémon, meaning the dataset has 801 observations, one for each pokémon. However, note that we removed one pokemon from the original 801 pokemon, Minior, from the dataset, since it has 2 different forms and has an uninterpretable capture rate. We noticed that while reading the .csv file, R automatically translated the \*capture_rate\* variable to characters because Minior's capture rate was listed as: "30 (Meteorite)255 (Core)". We decided to exclude this observation from the model because of its uninterpretable characteristics, and after, we casted \*capture_rate\* an integer instead of a character.

The dataset contains 23 variables, taken directly from the kaggle website for the dataset, explanations of which can be viewed in our [data dictionary](./data/README.md). For our analysis, we will be focusing on these specific variables:

**RESPONSE VARIABLE**

-   base_total: total base stats of the Pokemon \[whole number\]

**PREDICTOR VARIABLES**

-   experience_growth: The Experience Growth of the Pokemon \[whole number\]

-   base_egg_steps: The number of steps required to hatch an egg of the Pokemon \[whole number\]

-   base_happiness: Base Happiness of the Pokemon \[whole number\]

-   capture_rate: Capture Rate of the Pokemon \[whole number\]

-   experience_growth: The Experience Growth of the Pokemon \[whole number\]

-   generation: The numbered generation which the Pokemon was first introduced \[whole number 1-7\]

-   height_m: Height of the Pokemon \[number in meters\]

-   percentage_male: The percentage of the species that are male. \[percentage value, blank if the Pokemon is genderless\]

-   pokedex_number: The entry number of the Pokemon in the National Pokedex \[whole number between 1 and 801\]

-   type1: The Primary Type of the Pokemon (every pokémon has this)

-   type2: The Secondary Type of the Pokemon (not all pokémon have this)

-   weight_kg: The Weight of the Pokemon \[number in kilograms\]

-   is_legendary: Denotes if the Pokemon is legendary.\[0 = not legendary, 1 = legendary\]

As you'll notice, the dataset splits the base stats of each pokemon into the individual stats, but we only need to know about the base_total variable, which is included in the csv file. Each variable describes the pokémon at hand in a different way. Some, like is_legendary, may prove to be extremely important in our regression model, while with others, like name and Japanese name, we can remove them from consideration as they are simply unique identifiers.

```{r expdata1, warning = F, echo = F}
p1 <- ggplot(pokemon, aes(x = base_total)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black") +
  geom_vline(xintercept = mean(pokemon$base_total), color = "red", size = 1) +
  labs(title = "Distribution of Base Total", x = "Base Total")
```

```{r expdata3, warning = F, echo = F}
generation_means <- pokemon |>
  group_by(generation) |>
  summarise(mean_base_total = mean(base_total)) |>
  arrange(desc(mean_base_total))

p3 <- ggplot(generation_means, aes(x = as.factor(generation), y = mean_base_total)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(title = "Mean Base Total Across Generations",
       x = "Generation",
       y = "Mean Base Total")
```

```{r expdata4, warning = F, echo = F}
p4 <- ggplot(pokemon, aes(x = weight_kg, y = base_total)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = "blue") +
  labs(title = "Scatter Plot of Base Total vs. Weight", x = "Weight (kg)", y = "Base Total")
```

```{r expdata6, warning = F, echo = F}
pokemon$is_legendary <- as.factor(pokemon$is_legendary)
p6 <- ggplot(pokemon, aes(x = weight_kg, y = base_total, color = is_legendary)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Interaction Between Weight and Legendary Status", x = "Weight (kg)", y = "Base Total")
```

```{r plots, message = F, warning = F, echo = F}
p1 <- p1 + theme(
  plot.title = element_text(size = rel(0.8)), # Adjust plot title size
  axis.title.x = element_text(size = rel(0.8)),     # X axis label
  axis.title.y = element_text(size = rel(0.8)),     # Y axis label
  axis.text.x = element_text(size = rel(0.8)),      # X axis text
  axis.text.y = element_text(size = rel(0.8)),      # Y axis text
  legend.title = element_text(size = rel(0.8)),     # Legend title
  legend.text = element_text(size = rel(0.8))       # Legend text
)

p3 <- p3 + theme(
  plot.title = element_text(size = rel(0.8)), # Adjust plot title size
  axis.title.x = element_text(size = rel(0.8)),     # X axis label
  axis.title.y = element_text(size = rel(0.8)),     # Y axis label
  axis.text.x = element_text(size = rel(0.8)),      # X axis text
  axis.text.y = element_text(size = rel(0.8)),      # Y axis text
  legend.title = element_text(size = rel(0.8)),     # Legend title
  legend.text = element_text(size = rel(0.8))       # Legend text
)

p4 <- p4 + theme(
  plot.title = element_text(size = rel(0.8)), # Adjust plot title size
  axis.title.x = element_text(size = rel(0.8)),     # X axis label
  axis.title.y = element_text(size = rel(0.8)),     # Y axis label
  axis.text.x = element_text(size = rel(0.8)),      # X axis text
  axis.text.y = element_text(size = rel(0.8)),      # Y axis text
  legend.title = element_text(size = rel(0.8)),     # Legend title
  legend.text = element_text(size = rel(0.8))       # Legend text
)

p6 <- p6 + theme(
  plot.title = element_text(size = rel(0.8)), # Adjust plot title size
  axis.title.x = element_text(size = rel(0.8)),     # X axis label
  axis.title.y = element_text(size = rel(0.8)),     # Y axis label
  axis.text.x = element_text(size = rel(0.8)),      # X axis text
  axis.text.y = element_text(size = rel(0.8)),      # Y axis text
  legend.title = element_text(size = rel(0.8)),     # Legend title
  legend.text = element_text(size = rel(0.8))       # Legend text
)


plot_combined <- (p1 | p3) /
(p4 | p6)

plot_final <- plot_combined + 
              plot_layout(widths = c(1, 1, 1), heights = c(1, 1))

plot_combined
```

```{r summaryTotal, message = F, warning = F, echo = F}
tidy(summary(pokemon$base_total)) |>
  kable(digits=3)
```

**p1: Distribution of Base total:** We can see from the distribution of the base total, that is seems to be roughly trimodal with three peaks (one around 300, 400, and 475). The base total points vary from about 180 to 780 with a mean at around 428.4. The median is 435 which is higher than the mean by a little bit which could mean that there are more extreme low base total Pokemon bringing the mean down a little bit. The IQR is 185 (505 - 320) and looking at the data there might be a potential outlier around 780 base total.

**p2: Mean Base Total Across Generations:** This is the graph showing the relationship between different generation Pokemon and their mean base total. We can see from this graph that the mean base total of generation 4 Pokemon were the highest but overall we can't see any distinct relationships between the generation of the Pokemon and their mean base total. One can maybe say there could be a very slight positive linear relationship between the generation of the Pokemon and their mean base total as we obsere that as generation increases the mean base total tends to increase slightly.

**p3: Scatter Plot of Base Total vs Weight:** From the scatter plot I can see that there is no apparent correlation between the weight and the base total. There may be a very low positive correlation between these two variables but most of the points are focused around when the weight of the Pokemon is less than 100kg so it's hard to tell the relationship. There seems to be a few outliers near 900-1000kg.

**p4: Interaction Between Weight and Legendary Status:** This graph is showing the relationship between the weight of the Pokemon and the base total for legendary and non legendary Pokemon. The red line is for non legendary Pokemons (is_legendary = 0) and the blue line is for legendary Pokemon (is_legendary = 1). Something interesting we can note is that legendary Pokemon all tend to have a much higher and consistent base total for all weights. Across all weights most base total for legendary Pokemons stay between 550- 700. Among the legendary Pokemon there doesn't seem to be any correlation between the weight and the base total. Among the non-legendary Pokemon however, there seems to be a very slight positive correlation between the weight of the Pokemon and the base total. As the weight of the non-legendary Pokemon goes up, the Base total seems to go up slightly although with the cluster of points under 100 kg Pokemon, it is hard to conclude, and regardless, the correlation does not appear to be strong between these variables.

Note that from this graph we can tell that Legendary Pokemon appear to tend to have much higher base stat totals than non-legendary Pokemon on average, which is something that will definitely impact our multiple linear regression model.

# Methodology:

We are conducting a multiple linear regression model to predict base_total from several other predictor variables. Any form of logistic regression would not make sense in this case as base_total is a quantitative variable, meaning we are not making classifications, and thus MLR is the model we conduct.

```{r}
set.seed(123)
pokemon_split <- initial_split(pokemon)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```

We first randomly split our data of 800 different pokemon into 75% training and 25% testing data so we can both train and evaluate our model.

```{r}
pokemon_rec_full <- recipe(base_total ~ ., data = pokemon_train) |> #make name's role ID 
  update_role(name, new_role = "ID") |> 
  #remove abilities 
  step_rm(abilities) |> 
  #remove all "against" variables to eliminate redundancy & collinearity 
  step_rm(against_bug, against_dark, against_dragon, against_electric, against_fairy, against_fight, against_fire, against_flying, against_ghost, against_grass, against_ground, against_ice, against_normal, against_poison, against_psychic, against_rock, against_steel, against_water ) |>
  #remove types (for now)
   step_rm(type1, type2) |>
  #remove classification 
  step_rm(classfication) |> 
  #remove all individual stats 
  step_rm(attack, defense, hp, sp_attack, sp_defense, speed) |> 
  #remove japanese name 
  step_rm(japanese_name) |>
  step_rm(percentage_male) |>
    step_normalize(experience_growth) |>
   step_center(experience_growth,
              base_egg_steps,
              base_happiness,
              capture_rate,
              experience_growth,
              height_m,
              pokedex_number,
              weight_kg,
              generation) |>
  step_naomit(height_m)
```

Next, we take the training data through a recipe to ready it for our analysis. We took it through the following steps:

1.  We update the role of "name" to be an ID. Name is simply a label for each observation.

2.  We remove irrelevant, non-predicting information like abilities, classification, and Japanese name.

    -   There are hundreds of different abilities a Pokemon can have, and very little overlap of abilities between Pokemon, so we do not need the abilities variable. The classification of a Pokemon is almost unique for every Pokemon (there is very little overlap), and it mainly just groups Pokemon by their evolution line, yielding classification unwanted.

3.  We remove all against\_\* variables and all type variables.

    -   These two variables convey the same information. Since there are 18 types, including typing as a predictor would convolute the other predictor variables, leaving our model extremely difficult to interpret, so we continue our analysis without considering Pokemon typing.

4.  We mean-center all quantitative predictors so our intercept is interpretable.

5.  We address missing data in the height/weight category:

    -   There are 20 Pokemon with missing height and weight values. Per the author of the database, these 20 Pokemon have alternate regional forms where their height and weights differ from their normal form. Additionally, the other data about these Pokemon are based on the alternate form, creating a disparity between these 20 Pokemon and the rest of the Pokemon in the dataset. Thus, we decided to remove these 20 Pokemon from consideration.

6.  Finally, we remove percentage_male from consideration.

    -   Several Pokemon do not have a gender, leading to many missing values in the dataset. Upon further examination, we found that a disproportionate 63/70 of the legendary Pokemon in the dataset to not have a gender, due to the distinct and rare nature of the legendary status in the game, whereas most non-legendary pokemon do have a gender Thus, percentage_male and is_legendary cannot be effective predictors in conjunction, so we decide to remove percentage_male from our model.

```{r}
pokemon_spec_full <- linear_reg() |>
  set_engine("lm")

pokemon_wflow_full <- workflow() |>
  add_model(pokemon_spec_full) |>
  add_recipe(pokemon_rec_full)
```

After bringing the training data through our recipe, we fit the data under a multiple linear regression model, which is outputted below:

```{r}
pokemon_fit_full <- pokemon_wflow_full |>
  fit(data = pokemon_train)

tidy(pokemon_fit_full) |>
  kable(digits = 3)
```

As you can see from above base_egg_steps, base_happiness, and weight_kg are three candidates for variables to remove from the model, since we notice their p-values are all greater than 0.05, meaning they are potentially statistically insignificant predictors. Thus, the next step in our method is compare two models through a series of tests: the one model being our original model with all predictors after running our feature engineering, and one with base_egg_steps, base_happiness, and weight_kg removed to select a model that is either concise with enough comprehensiveness or as comprehensive as we can make it.

```{r}
pokemon_rec_less <- recipe(base_total ~ ., data = pokemon_train) |> #make name's role ID 
  update_role(name, new_role = "ID") |> 
  #remove abilities 
  step_rm(abilities) |> 
  #remove all "against" variables to eliminate redundancy & collinearity 
  step_rm(against_bug, against_dark, against_dragon, against_electric, against_fairy, against_fight, against_fire, against_flying, against_ghost, against_grass, against_ground, against_ice, against_normal, against_poison, against_psychic, against_rock, against_steel, against_water ) |>
  #remove types (for now)
   step_rm(type1, type2) |>
  #remove classification 
  step_rm(classfication) |> 
  #remove all individual stats 
  step_rm(attack, defense, hp, sp_attack, sp_defense, speed) |> 
  #remove japanese name 
  step_rm(japanese_name) |>
  step_rm(percentage_male) |>
  #center all quantitative predictors
  step_center(experience_growth,
              base_egg_steps,
              base_happiness,
              capture_rate,
              experience_growth,
              height_m,
              pokedex_number,
              weight_kg,
              generation) |>
  step_rm(base_egg_steps, 
          base_happiness, 
          weight_kg)
```

```{r}
pokemon_spec_less <- linear_reg() |>
  set_engine("lm")

pokemon_wflow_less <- workflow() |>
  add_model(pokemon_spec_less) |>
  add_recipe(pokemon_rec_less)
```

We run the training data through a very similar recipe for the second model except that base_egg_steps, base_happiness, and weight_kg are removed. The output of running multiple linear regression for this model is shown below:

```{r}
pokemon_fit_less <- pokemon_wflow_less |>
  fit(data = pokemon_train)

tidy(pokemon_fit_less) |>
  kable(digits = 3)
```

We then conducted two tests to decipher which model is best to select and use for our final model: 1 comparing the AIC, BIC, and adjusted r squared for the models, and another comparing the results of V-fold cross validation for the two models. The output for the first test is shown below:

```{r}
glance(pokemon_fit_full) |> 
  select(AIC, BIC, adj.r.squared) |>
  kable(digits = 3)

glance(pokemon_fit_less) |> 
  select(AIC, BIC, adj.r.squared) |>
  kable(digits = 3)
```

From this test, we note that both AIC and BIC are lower for the second, reduced model than the first model. The adjusted r squared values are very similar in size. With this information, it would be logical to select the second model, as it produces more preferable values of AIC and BIC while maintaining a very similar adjusted r squared value.

However, we run one more test, v-fold cross validation, to compare the models once again to have another point of evidence to base our decision off of. The results of this second test are below:

Cross validation results for the 1st, full model:

```{r}
set.seed(123)
folds <- vfold_cv(pokemon_train, v = 15)

pokemon_wflowV_full <- workflow() |>
	add_model(pokemon_spec_full) |>
	add_recipe(pokemon_rec_full)

pokemon_fitV_full <- pokemon_wflowV_full |>
	fit_resamples(folds)
collect_metrics(pokemon_fitV_full) |>
  kable(digits = 3)
```

Cross validation for the 2nd, reduced model:

```{r}
pokemon_wflowV_less <- workflow() |>
	add_model(pokemon_spec_less) |>
	add_recipe(pokemon_rec_less)

pokemon_fitV_less <- pokemon_wflowV_less |>
	fit_resamples(folds) 
collect_metrics(pokemon_fitV_less) |>
  kable(digits = 3)
```

We notice that the RMSE value from the cross validation of the second model is lower than the RMSE value from the cross validation of the first full model, while the r squared values from both models have negligible difference, leading us to believe that the second model performs better in terms of predicting a pokemon's base stat total.

From the results of these two tests, **we can confidently select model number 2, the reduced model**.

The final check of our methodology is to examine the VIF values for our predictors to confirm there is no collinearity present in our model:

```{r}
#getting VIF numbers. Only worry about multicolinearity if VIFS are greater than 10
pokemon_fitless <- extract_fit_parsnip(pokemon_fit_less)
vif(pokemon_fitless$fit) |>
  tidy() |>
  kable(digits = 3)
```

A VIF value greater than 10 for a variable indicates concerning collinearity. We notice that pokedex_number and generation both have VIF values \> 40, while all other variables have very low VIF values, meaning pokedex_number and generation appear to depend heavily on each other. This makes perfect sense -- as generations of pokemon are simply intervals of pokedex numbers, or in other words, generation divides all values 1-800 of pokedex_number into different intervals (for example, generation 1 is pokedex numbers 1-151). Thus, we should remove 1 of the 2 variables before continuing with our final model. We choose to delete generation, since it is simply a far more discrete form of pokedex_number.

Our final model is the same as the second model with generation removed.

```{r}
pokemon_rec_final <- recipe(base_total ~ ., data = pokemon) |> #make name's role ID 
  update_role(name, new_role = "ID") |> 
  #remove abilities 
  step_rm(abilities) |> 
  #remove all "against" variables to eliminate redundancy & collinearity 
  step_rm(against_bug, against_dark, against_dragon, against_electric, against_fairy, against_fight, against_fire, against_flying, against_ghost, against_grass, against_ground, against_ice, against_normal, against_poison, against_psychic, against_rock, against_steel, against_water ) |>
  #remove types (for now)
   step_rm(type1, type2) |>
  #remove classification 
  step_rm(classfication) |> 
  #remove all individual stats 
  step_rm(attack, defense, hp, sp_attack, sp_defense, speed) |> 
  #remove japanese name 
  step_rm(japanese_name) |>
  step_rm(percentage_male) |>
  #center all quantitative predictors
  step_center(experience_growth,
              base_egg_steps,
              base_happiness,
              capture_rate,
              experience_growth,
              height_m,
              pokedex_number,
              weight_kg,
              generation) |>
  step_normalize(experience_growth) |>
  step_rm(base_egg_steps, 
          base_happiness, 
          weight_kg,
          generation)
```

```{r}
pokemon_spec_final <- linear_reg() |>
  set_engine("lm")

pokemon_wflow_final <- workflow() |>
  add_model(pokemon_spec_final) |>
  add_recipe(pokemon_rec_final)
```

```{r}
pokemon_fit_final <- pokemon_wflow_final |>
  fit(data = pokemon)
```

```{r}
#getting VIF numbers. Only worry about multicolinearity if VIFS are greater than 10
pokemon_fitless <- extract_fit_parsnip(pokemon_fit_final)
vif(pokemon_fitless$fit) |>
  tidy(kable(digits = 3))
```

As we can see, the VIF values are all now satisfactorily low for the final model, which we will display below:

```{r}
tidy(pokemon_fit_final) |>
  kable(digits = 3)
```

The equation for the final model would be

$$
base\_total = \beta_0 + \beta_1 * capture\_rate + \beta_2* experience\_growth \\ + \beta_3 * height\_m + \beta_4 * pokedex\_number + \beta_5 * is\_legendary1
$$

Filling the equation in we have

$$
base\_total = 419.982 - 0.839 * capture\_rate + 2.412* experience\_growth \\ + 28.62 * height\_m + 0.024 * pokedex\_number + 88.596 * is\_legendary1
$$

# Results:

We first check the Model Conditions

**Model Conditions: Linearity / Constant Variance / Normality**

```{r}
pokemon_fit_model123 <- extract_fit_parsnip(pokemon_fit_final)
pokemon_aug <- augment(pokemon_fit_model123$fit)

p1 <- ggplot(data = pokemon_aug, aes(x = capture_rate, y = .resid)) +
	geom_point() +
	geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Capture Rate", y = "Residuals", title = "Residuals vs Capture Rate")
p2 <- ggplot(data = pokemon_aug, aes(x = experience_growth, y = .resid)) +
	geom_point() +
	geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Experience Growth", y = "Residuals", title = "Residuals vs Experience Growth")
p3 <- ggplot(data = pokemon_aug, aes(x = height_m, y = .resid)) +
	geom_point() +
	geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Height (m)", y = "Residuals", title = "Residuals vs Height(m)")
p4 <- ggplot(data = pokemon_aug, aes(x = pokedex_number, y = .resid)) +
	geom_point() +
	geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Pokedex Number", y = "Residuals", title = "Residuals vs Pokedex Number")
p5 <- ggplot(data = pokemon_aug, aes(x = is_legendary, y = .resid)) +
	geom_point() +
	geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "is Legendary", y = "Residuals", title = "Residuals vs is_Legendary")
norm <- ggplot(data = pokemon_aug, aes(x = .resid)) +
  geom_histogram() + 
  labs(title = "Distribution of residuals")
(p1 + p2) / (p3 + p4) / (p5 + norm)

```

**Linearity condition** - This condition is satisfied as there is no clear patterns in the residuals vs predictor variables such as a fanning pattern.

**Constant Variance** - The vertical spread of the residuals is constant across the plots of residuals versus fitted values, therefore this condition is satisfied.

**Normality** - The distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size is sufficiently large \>\> 30 so we can relax this condition.

**Independence** - The independence condition is **not** satisfied. We established earlier there is little variation in base total over generations, anaogous to time in Pokemon, ridding the worry for a serial effect. However, due to the evolutionary nature of Pokemon, there is non-independent data in our data set. Most Pokemon evolve from a different pokemon/evolve into a new Pokemon as they gain experience, and these Pokemon among the same evolutionary line are not independent of eachother. For example, Bulbasaur, Ivysaur, and Venusaur are all among the same evolutionary line and they often have the same or similar typing (though we aren't considering this in our model) and generation while often maintaining correlated base stat totals and other information. Thus, our data does not satisfy the independence condition.

**Limitations**

Due to the non-independence of some observations in our dataset, our model will not be as good of a predictor compared to one over completely independent Pokemon as it assumes independent error terms when that is not the case. Instead of manually removing all but one pokemon in the each evolutionary line, we decide to keep the data as is with the non-independent limitation in mind as it better reflects the point of our model. If we decided to alter our data set to make it pass the independence condition, we would be removing more than half of our observations, and we would draw far less relevant conclusions in regards to our research question. We want to see how well different variables can predict the base stat total of *any given pokemon*, not just the pokemon at the top of their evolutionary chain. Thus, we accept that the lack of independence negatively affects the model, but we carry on with conclusions and results as is.

**Results**

```{r}
pokemon_fit_final <- pokemon_wflow_final |>
	fit_resamples(folds)
collect_metrics(pokemon_fit_final) |>
  kable(digits=3)
```

```{r}
pokemon_final_fit_train <- pokemon_wflow_final |>
  fit(data = pokemon_train)

pokemon_train_pred <- predict(pokemon_final_fit_train, pokemon_train) |>
  bind_cols(pokemon_train)

rsq(pokemon_train_pred, truth = base_total, estimate = .pred)
rmse(pokemon_train_pred, truth = base_total, estimate = .pred)
```

```{r}
pokemon_test_pred <- predict(pokemon_final_fit_train, pokemon_test) |>
  bind_cols(pokemon_test)

rsq(pokemon_test_pred, truth = base_total, estimate = .pred)
rmse(pokemon_test_pred, truth = base_total, estimate = .pred)
```

When we compare the $R^2$ values, that of the training model was .641 while the testing model had a $R^2$ value of .577. This is to be expected since the training data has been trained on so it should be a better predictor than the testing model. When we compare the RMSE for the training and testing data, we can see that the RMSE for the training set was 71.1 while the RMSE of the testing set was 80.69. Since lower RMSE values indicate a better fit to the model and we can see that the RMSE of the testing set was greater than that of the training set which means the training data once again is a better predictor and this is once again to be expected since training data was trained on so it should predict the model better. Since the difference in the $R^2$ and the RMSE values between the training and the testing data wasn't too significant, this shows that our model doesn't over fit the data.

Overall, for our initial interpretations of our final model, we find it to be a relatively strong predictor for base_total of a Pokemon given the relatively low RMSE value and the relatively high $R^2$ value. Thus, we have confirmed our hypothesis that in fact, we *can* predict the base_total of a Pokemon with decent accuracy by using different variables.

# Discussion + Conclusion:

What we have learned:

In this section you'll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.
